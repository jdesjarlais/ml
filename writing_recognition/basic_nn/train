#!/usr/bin/env python3

# Trains an optimal margin classifier on the mnist date set.

import numpy as np
import torch
import time
import math
from typing import List, Optional

from torchvision import transforms, utils
from torch.utils.data import Dataset, DataLoader



######### SETTINGS #########
should_train = True
learning_rate = 0.001
epochs = 10

training_set_size = 60000
test_set_size = 10000
training_batch_size = 1000

training_labels_file = "/Users/adesjarlais/devel/data/mnist/train-labels-idx1-ubyte"
training_images_file = "/Users/adesjarlais/devel/data/mnist/train-images-idx3-ubyte"

test_labels_file = "/Users/adesjarlais/devel/data/mnist/t10k-labels-idx1-ubyte"
test_images_file = "/Users/adesjarlais/devel/data/mnist/t10k-images-idx3-ubyte"

print_metrics = True
metrics_print_interval = 1
model_save_file = "/Users/adesjarlais/devel/data/models/writing_recognition/basic_nn/model.pt"
save_model = True
use_saved_model = True
############################

assert(training_set_size <= 60000)
assert(training_batch_size <= training_set_size)
assert(test_set_size <= 10000)



# Transforms

class Normalize(object):
    """Normalize a tensor to a min/max range."""

    __input_range = 1.0
    __output_range = 1.0

    def __init__(self, input_min, input_max, output_min=0.0, output_max=1.0):
        self.input_min = input_min
        self.__input_range = input_max - input_min

        self.output_min = output_min
        self.__output_range = output_max - output_min

    def __call__(self, sample):
        assert(type(sample) is torch.Tensor)

        normalized_sample = (sample - self.input_min)/self.__input_range
        return (normalized_sample * self.__output_range) + self.output_min

class ToTensor(object):
    """Convert ndarrays in sample to Tensors."""

    def __call__(self, sample):
        tensor = None
        if type(sample) is np.ndarray:
            tensor = torch.from_numpy(sample)
        else:
            tensor = torch.Tensor([sample])
        return tensor

class ToOneHot(object):
    """Converts a constant or list of constants to a one hot array"""

    def __init__(self, num_classes):
        self.num_classes = num_classes

    def __call__(self, sample):
        output = None
        if type(sample) is np.ndarray:

            output = -1 * torch.ones((sample.shape[0], self.num_classes))
            for i in range(sample.shape[0]):
                hot_index = sample[i]
                output[i][hot_index] = 1
        elif type(sample) is torch.Tensor:
            output = -1 * torch.ones((sample.shape[0], self.num_classes))
            for i in range(sample.shape[0]):
                output[i][sample] = 1
        else:
            output = torch.zeros(self.num_classes)
            output[sample] = 1
        return output


# Datasets

class MNISTDataset(Dataset):

    images_file_path = None
    labels_file_path = None

    image_size = None

    __has_been_loaded = False

    __len = None
    __images = None
    __labels = None

    def __init__(self, images_file_path, labels_file_path, input_transform=None, label_transform=None):
        self.images_file_path = images_file_path
        self.labels_file_path = labels_file_path

        self.input_transform = input_transform
        self.label_transform = label_transform

    def __len__(self):
        self.__load_data_set_if_needed()

        return self.__len

    def __getitem__(self, idx):
        self.__load_data_set_if_needed()

        image = self.__images[idx]
        if self.input_transform:
            image = self.input_transform(image)

        label = self.__labels[idx]
        if self.label_transform:
            label = self.label_transform(label)

        return image, label

    def __load_data_set_if_needed(self):
        if not self.__has_been_loaded:
            self.__load_data_set()

    def __load_data_set(self):
        assert(not self.__has_been_loaded)

        self.__has_been_loaded = True

        with open(self.images_file_path, "r") as images_file:
            header = np.fromfile(images_file, dtype=np.dtype('>i4'), count=4)

            magic_number = header[0]
            item_count = header[1]
            num_rows = header[2]
            num_columns = header[3]
            pixels_per_image = num_rows * num_columns
            images_bytes = np.fromfile(images_file, dtype=np.ubyte, count=-1)

            images_file.close()

            assert(magic_number == 2051)
            assert(num_rows == 28)
            assert(num_columns == 28)
            assert(images_bytes.size == item_count * num_rows * num_columns)

            images = np.reshape(images_bytes, (item_count, -1))

            # TODO: is there a way to apply Transforms to a list or inputs

            self.image_size = num_rows * num_columns
            self.__images = images

        with open(self.labels_file_path, "r") as labels_file:
            header = np.fromfile(labels_file, dtype=np.dtype('>i4'), count=2)

            magic_number = header[0]
            item_count = header[1]
            labels = np.fromfile(labels_file, dtype=np.ubyte, count=-1)

            labels_file.close()

            assert(magic_number == 2049)
            assert(labels.size == item_count)

            self.__labels = labels

        assert(self.__images.shape[0] == self.__labels.shape[0])

        self.__len = self.__images.shape[0]


# Model

class BasicFeedForwardModel(torch.nn.Module):

    def __init__(self, input_size, output_size):
        super(BasicFeedForwardModel, self).__init__()

        self.layers = torch.nn.Sequential(
            torch.nn.Linear(input_size, 28),
            torch.nn.Sigmoid(),
            torch.nn.Linear(28, output_size),
            torch.nn.Softmax(0)
        )

    def forward(self, x):
        return self.layers(x)



# Functions
 
def save_model_state(model, output_file_path):
    save_model_state_dict(model.state_dict(), output_file_path)

def load_model_state(model, file_path):
    try:
        model.load_state_dict(load_model_state_dict(file_path))
    except:
        print("Failed to set model parameters.")

def save_model_state_dict(model_state_dict, output_file_path):
    try:
        torch.save(model_state_dict, output_file_path)
    except:
        print("Failed to save model parameters.")

def load_model_state_dict(file_path):
    try:
        return torch.load(file_path)
    except:
        print("Failed to load model parameters.")

    return None

def calculate_model_accuracy(model, inputs, expected_outputs):
    model_outputs = model(inputs)
    model_outputs = torch.argmax(model_outputs, dim=1)

    correct_outputs = model_outputs == expected_outputs
    total_correct = torch.sum(correct_outputs).item()

    return total_correct/inputs.shape[0]


# Training

device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"Using device: {device}")


input_transform = transforms.Compose([
    ToTensor(),
    Normalize(input_min=0, input_max=255, output_min=0.0, output_max=1.0)
])

label_transform = transforms.Compose([
    ToOneHot(10)
])

mnist_train_dataset = MNISTDataset(images_file_path=training_images_file,
                                   labels_file_path=training_labels_file,
                                   input_transform=input_transform,
                                   label_transform=label_transform)
mnist_test_dataset = MNISTDataset(images_file_path=test_images_file,
                                  labels_file_path=test_labels_file,
                                  input_transform=input_transform,
                                  label_transform=label_transform)

mnist_train_dataloader = DataLoader(mnist_train_dataset, batch_size=training_batch_size, shuffle=True, num_workers=0, pin_memory=True)
mnist_test_dataloader = DataLoader(mnist_test_dataset, shuffle=True, num_workers=0, pin_memory=True)




train_X, train_Y = mnist_train_dataset[:training_set_size]
test_X, test_Y = mnist_test_dataset[:test_set_size]
assert(train_X.shape[0] == train_Y.shape[0])
assert(test_X.shape[0] == test_Y.shape[0])

train_X = train_X.to(device)
train_Y = train_Y.to(device)
test_X = test_X.to(device)
test_Y = test_Y.to(device)

input_size = train_X.shape[1]
model = BasicFeedForwardModel(input_size, 10)
model.to(device)

loss_func = torch.nn.CrossEntropyLoss()




if use_saved_model:
    load_model_state(model, model_save_file)


optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)



start_time = time.time()
if should_train:

    train_Y_argmax = torch.argmax(train_Y, 1).to(device)
    test_Y_argmax = torch.argmax(test_Y, 1).to(device)

    assert(training_set_size % training_batch_size == 0)

    num_batches = int(training_set_size/training_batch_size)


    for epoch in range(epochs):
        train_example_count = 0
        cumulative_loss = 0.0

        forward_time = 0.0
        loss_time = 0.0
        backward_time = 0.0


        for batch, (batch_X, batch_Y) in enumerate(mnist_train_dataloader):
            batch_X = batch_X.to(device)
            batch_Y = batch_Y.to(device)

            optimizer.zero_grad()

            forward_start = time.time()
            model_outputs = model(batch_X)
            forward_end = time.time()
            forward_time += forward_end - forward_start

            loss_start = time.time()
            loss = loss_func(model_outputs, batch_Y)
            loss_end = time.time()
            loss_time += loss_end - loss_start

            backward_start = time.time()
            loss.backward()
            optimizer.step()
            backward_end = time.time()
            backward_time += backward_end - backward_start

            cumulative_loss += torch.sum(loss).item()
            train_example_count += batch_X.shape[0]


        average_loss = cumulative_loss/train_example_count

        if not print_metrics or epoch % metrics_print_interval != 0:
            print(f"Epoch: {epoch}")
            print(f"\tloss: {average_loss:0.20f}")
            print(f"\tforward time: {forward_time*1000:0.3f}ms\t")
            print(f"\tloss time: {loss_time*1000:.3f}ms")
            print(f"\tbackward time: {backward_time*1000:0.3f}ms")
            print()
        else:
            metrics_start = time.time()

            train_average_accuracy = 0
            test_average_accuracy = 0

            with torch.no_grad():
                train_average_accuracy = calculate_model_accuracy(model, train_X, train_Y_argmax)
                test_average_accuracy = calculate_model_accuracy(model, test_X, test_Y_argmax)

            metrics_end = time.time()
            metrics_time = metrics_end - metrics_start


            # Print metrics
            print(f"Epoch: {epoch}")
            print(f"\tloss: {average_loss:0.20f}\ttrain_accuracy: {train_average_accuracy:.4f}\ttest_accuracy: {test_average_accuracy:.4f}")
            print(f"\tforward time: {forward_time*1000:0.3f}ms\t")
            print(f"\tloss time: {loss_time*1000:.3f}ms")
            print(f"\tbackward time: {backward_time*1000:0.3f}ms")
            print(f"\tmetrics time: {metrics_time*1000:.3f}ms")
            print()

    # Print losses one last time
    print("Training complete.")
    print("")



if save_model:
    save_model_state(model, model_save_file)




# Calcualte final results

loss = 0.0
train_average_accuracy = 0.0
train_average_accuracy = 0.0

with torch.no_grad():
    model_outputs = model(train_X)
    losses = loss_func(model_outputs, train_Y)
    loss = torch.sum(losses).item()/train_X.shape[0]

    train_average_accuracy = calculate_model_accuracy(model, train_X, train_Y_argmax)
    test_average_accuracy = calculate_model_accuracy(model, test_X, test_Y_argmax)

print("FINAL RESULTS:")
print(f"Loss: {loss:0.5f}\ttrain_accuracy: {train_average_accuracy:0.4f}\ttest_accuracy: {test_average_accuracy:0.4f}")


end_time = time.time()
print("Total execution time: {:.2f}".format(end_time - start_time))
