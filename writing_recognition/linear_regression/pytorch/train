#!/usr/bin/env python3

# Finds a linear regression model using the normal equation for the mnist date set.

import numpy as np
import torch
import time

from torchvision import transforms, utils
from torch.utils.data import Dataset, DataLoader

import warnings
warnings.filterwarnings("error")


######### SETTINGS #########
should_train = True
learning_rate = 0.00007
epochs = 10
training_batch_size = 200
test_batch_size = 100

training_labels_file = "/Users/adesjarlais/devel/data/mnist/train-labels-idx1-ubyte"
training_images_file = "/Users/adesjarlais/devel/data/mnist/train-images-idx3-ubyte"

test_labels_file = "/Users/adesjarlais/devel/data/mnist/t10k-labels-idx1-ubyte"
test_images_file = "/Users/adesjarlais/devel/data/mnist/t10k-images-idx3-ubyte"

model_save_file = "/Users/adesjarlais/devel/data/models/writing_recognition/linear_regression/model.pt"
save_model = True
use_saved_model = False
############################


def save_model(model_params, output_file_path):
    torch.save(model_params, output_file_path)

def load_model(file_path):
    return torch.load(file_path)
    return np.loadtxt(file_path, dtype=float)


# Transforms

class Normalize(object):
    """Normalize a tensor to a min/max range."""

    __input_range = 1.0
    __output_range = 1.0

    def __init__(self, input_min, input_max, output_min=0.0, output_max=1.0):
        self.input_min = input_min
        self.__input_range = input_max - input_min

        self.output_min = output_min
        self.__output_range = output_max - output_min

    def __call__(self, sample):
        # TODO: assert sample is pytorch tensor
        #assert isinstance(, (int, tuple))

        normalized_sample = (sample - self.input_min)/self.__input_range

        return (normalized_sample * self.__output_range) + self.output_min

class ToTensor(object):
    """Convert ndarrays in sample to Tensors."""

    def __call__(self, sample):
        tensor = None
        if type(sample) is np.ndarray:
            tensor = torch.from_numpy(sample)
        else:
            tensor = torch.Tensor([sample])
        return tensor

class PrependConstant(object):
    """Insert a single constant at the beginning of a tensor"""

    def __init__(self, constant):
        self.constant = constant

    def __call__(self, sample):
        # TODO: assert sample is numpy array
        images = np.insert(sample, 0, self.constant, axis=-1)
        return images


# Datasets

class MNISTDataset(Dataset):

    images_file_path = None
    labels_file_path = None

    __has_been_loaded = False

    __len = None
    __images = None
    __labels = None

    def __init__(self, images_file_path, labels_file_path, input_transform=None, label_transform=None):
        self.images_file_path = images_file_path
        self.labels_file_path = labels_file_path

        self.input_transform = input_transform
        self.label_transform = label_transform

    def __len__(self):
        self.__load_data_set_if_needed()

        return self.__len

    def __getitem__(self, idx):
        self.__load_data_set_if_needed()

        image = self.__images[idx]
        if self.input_transform:
            image = self.input_transform(image)

        label = self.__labels[idx]
        if self.label_transform:
            label = self.label_transform(label)

        return image, label

    def __load_data_set_if_needed(self):
        if not self.__has_been_loaded:
            self.__load_data_set()

    def __load_data_set(self):
        assert(not self.__has_been_loaded)

        self.__has_been_loaded = True

        with open(self.images_file_path, "r") as images_file:
            header = np.fromfile(images_file, dtype=np.dtype('>i4'), count=4)

            magic_number = header[0]
            item_count = header[1]
            num_rows = header[2]
            num_columns = header[3]
            pixels_per_image = num_rows * num_columns
            images_bytes = np.fromfile(images_file, dtype=np.ubyte, count=-1)

            images_file.close()

            assert(magic_number == 2051)
            assert(num_rows == 28)
            assert(num_columns == 28)
            assert(images_bytes.size == item_count * num_rows * num_columns)

            images = np.reshape(images_bytes, (item_count, -1))

            # TODO: is there a way to apply Transforms to a list or inputs

            self.__images = images

        with open(self.labels_file_path, "r") as labels_file:
            header = np.fromfile(labels_file, dtype=np.dtype('>i4'), count=2)

            magic_number = header[0]
            item_count = header[1]
            labels = np.fromfile(labels_file, dtype=np.ubyte, count=-1)

            labels_file.close()

            assert(magic_number == 2049)
            assert(labels.size == item_count)

            self.__labels = labels

        assert(self.__images.shape[0] == self.__labels.shape[0])

        self.__len = self.__images.shape[0]

class LinearRegressionModel(torch.nn.Module):

    def __init__(self, input_size, output_size):
        super(LinearRegressionModel, self).__init__()
        self.linear = torch.nn.Linear(input_size, output_size)

    def forward(self, x):
        x = self.linear(x)
        x = torch.reshape(x, (-1, ))
        return x



device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"Using device: {device}")


input_composed_transform = transforms.Compose([
    PrependConstant(255),
    ToTensor(),
    Normalize(input_min=0, input_max=255, output_min=0.0, output_max=9.0)
])

label_transform = ToTensor()

mnist_train_dataset = MNISTDataset(images_file_path=training_images_file,
                                   labels_file_path=training_labels_file,
                                   input_transform=input_composed_transform,
                                   label_transform=label_transform)
mnist_test_dataset = MNISTDataset(images_file_path=test_images_file,
                                  labels_file_path=test_labels_file,
                                  input_transform=input_composed_transform,
                                  label_transform=label_transform)

mnist_train_dataloader = DataLoader(mnist_train_dataset, batch_size=training_batch_size, shuffle=True, num_workers=0)
mnist_test_dataloader = DataLoader(mnist_test_dataset, batch_size=test_batch_size, shuffle=True, num_workers=0)

# TODO: don't hard code these numbers
linear_regression_model = LinearRegressionModel(input_size=785, output_size=1).to(device)

# TODO: write these custom
loss_function = torch.nn.MSELoss()
optimizer = torch.optim.SGD(linear_regression_model.parameters(), lr=learning_rate)


# Grab the entire test data set
test_inputs, test_labels = mnist_test_dataset[:]
test_inputs = test_inputs.to(device)
test_labels = test_labels.to(device)

start_time = time.time()
for epoch in range(epochs):
    for i, (inputs, labels) in enumerate(mnist_train_dataloader):
        linear_regression_model.train()

        inputs = inputs.to(device)
        labels = labels.to(device).float()
        labels = torch.reshape(labels, (-1, ))

        outputs = linear_regression_model(inputs)
        losses = loss_function(outputs, labels)

        # TODO: Understand what is going on here!!
        # TODO: write custom versions of these functions
        losses.backward()
        optimizer.step()
        optimizer.zero_grad()

        loss = 0
        accuracy = 0
        with torch.no_grad():
            linear_regression_model.eval()

            loss = torch.sum(losses)

            test_set_outputs = linear_regression_model(test_inputs)
            actual_outputs = torch.clip(test_set_outputs, min=0.0, max=10.0)
            actual_outputs = actual_outputs.int()
            correct_values = torch.eq(actual_outputs, test_labels)
            total_correct = torch.sum(correct_values).item()
            accuracy = total_correct/test_inputs.shape[0]

        print(f"Epoch: {epoch}\tloss: {loss:0.2f}\taccuracy: {accuracy:0.2f}")


# Print losses one last time
print("Complete.")
print("FINAL RESULTS:")

end_time = time.time()
print("Total execution time: {:.2f}".format(end_time - start_time))
